#Models are rather used in back-end python scripts
#After evaluating the model apply it to Transform Data within Power Query

#1 - Think of removing outliers from the sample before creating the model

#Pearson to find the Correlation Coefficient and p-value
#input x and y as array
#output (Pearson coefficient, p-value) tuple
 
 import scipy as sp
 result = sp.stats.pearsonr( dataset['x'] , dataset['y'] )

#Correlation matrix of all variables

 result = dataset.corr()

#Linear Regression, x input may have the indexes being a 2D dataframe
 
 x = dataset[['x']]
 y = dataset['y']
 from sklearn.linear_model import LinearRegression
 lm = LinearRegression()
 lm.fit( x,y )
 result = pandas.DataFrame( {'Coefficient': lm.coef_ , 'Intercept': lm.intercept_ , 'R2': lm.score(x,y) } )

#Once the model is complete, to predict new values do

 predictions = lm.predict( dataset['new values'] )
 
#Multiple Linear regression, x's inputs may have the indexes being a 2D dataframe

 xs = dataset[['x1','x2','x3','x4','x5']]
 y = dataset['y']
 lm.fit( xs,y )
 result = pandas.DataFrame( {'Coefficients': lm.coef_, 'Intercept': lm.intercept_ , 'R2': lm.score(xs,y) } )

#Polynomial Regression of 3rd order

 import numpy as np
 from sklearn.metrics import r2_score
 
 x = dataset['x']
 y = dataset['y']
 f = np.polyfit( x , y , 3 )   #return an array with the 3 coefficients (highest first) and the intercept of the best fit
 y_predict = np.poly1d( f )    #return a numpy.poly1d equation 
 
 r2 = r2_score( y, y_predict(x) )
 result = pandas.DataFrame( {'Equation': y_predict , 'R2': r2} )
  
  
#Multiple Polynomial Regression of 2nd order 
#This one requires a bit more abstraction from linear algebra to understand
#Y = a + b1*x1 + b2*x2 + b3*x1x2 + b4*x1² + b5*x2² (for 2 variables -> 5 coefficients)
#First, we need to transform the features in polynomial equations before deploying the model

 xs = dataset[['x1','x2','x3','x4','x5']]
 
 from sklearn.linear_model import LinearRegression
 from sklearn.preprocessing import PolynomialFeatures
 pr = PolynomialFeatures(degree=2)
 xs_pr = pr.fit_transform( xs )
 
 #Now the data is linearized and we need to find the coefficients that best fit these equations to result in y
 poly = LinearRegression() 
 poly.fit( xs_pr , y )
 result = pandas.DataFrame( {'Coefficients': poly.coef_ , 'Intercept': poly.intercept_ , 'R2': poly.score(xs_pr,y) } )
 
#Pipeline to Multiple Polynomial Regression easier

 from sklearn.pipeline import Pipeline
 from sklearn.preprocessing import StandardScaler
 
 #We create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.
 Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]
 pipe=Pipeline(Input)
 
 xs = dataset[['x1','x2','x3','x4','x5']]
 y = dataset['y']
 pipe.fit( xs , y )
 result = pandas.DataFrame( {'Coefficients': lm_coef_ , 'Intercept': lm_intercept_ , 'R2': lm_score(xs_pr,y) } )
 
#Mean Squared Error (MSE) for Linear Regression

 from sklearn.metrics import mean_squared_error
 MSE = mean_squared_error(dataset['y'], y_predict)

#R-squared (R²) for Linear Regression

 from sklearn.linear_model import LinearRegression
 LinearRegression.score(X,y)

#Mean Squared Error (MSE) for Polynomial Fit

 from sklearn.metrics import mean_squared_error
 MSE = mean_squared_error( dataset['y'], y_predict(x) )

#R-squared (R²) for Polynomial Fit

 from sklearn.metrics import r2_score
 r2 = r2_score( dataset['y'], y_predict(x) )
 
#Out-of-sample evaluations
#1 - Splitting the data in train and test samples

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(dataset['x'], dataset['y'], test_size=0.15, random_state=1)

#R2 for the test data

lr = LinearRegression()
lr.fit(x_train, y_train)
lr.score(x_test, y_test)

#Cross-validation samples (ex: cv=4)

from sklearn.model_selection import cross_val_score
r2cross = cross_val_score(lr, x, y, cv=4 )
return = pandas.DataFrame( {'R2 in cvs samples': r2cross, 'R2 average': r2cross.mean() , 'R2 std deviation': r2cross.std()} )

#RidgeModel searchs to minimize the Least Squares, by damping the coefficients and avoid overfitting in the linear regression model

from sklearn.linear_model import Ridge
rr = Ridge(alpha = 0.1)
rr.fit(x_train , y_train)
return = rr.score(x_test, y_test)

#Using Grid Search to find the best hyperparameters like alpha for the Ridge 

from sklearn.model_selection import GridSearchCV
parameters = [{'alpha': [0.001, 0.1, 10, 100, 1000, 10000, 100000, 10000000]}]    #Add more if needed
rr=Ridge()
grid = GridSearchCV(rr, parameters, cv=4)
grid.fit(x_data, y_data)
best_rr = grid.best_estimator_

return = pandas.DataFrame( { 'Best model score': best_rr.score(x_test, y_test) } )


#GradientBoostingRegressor to optimize regressions
