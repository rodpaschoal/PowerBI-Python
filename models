#Models are rather used in back-end python scripts
#After evaluating the model apply it to Transform Data within Power Query

#1 - Think of removing outliers from the sample before creating the model

#Pearson to find the Correlation Coefficient and p-value
#input x and y as array
#output (Pearson coefficient, p-value) tuple
 
 import scipy as sp
 result = sp.stats.pearsonr( dataset['x'] , dataset['y'] )

#Correlation matrix of all variables

 result = dataset.corr()

#REGRESSION MODELS
#Linear Regression, x input may have the indexes being a 2D dataframe
 
 x = dataset[['x']]
 y = dataset['y']
 from sklearn.linear_model import LinearRegression
 lm = LinearRegression()
 lm.fit( x,y )
 result = pandas.DataFrame( {'Coefficient': lm.coef_ , 'Intercept': lm.intercept_ , 'R2': lm.score(x,y) } )

#Once the model is complete, to predict new values do

 predictions = lm.predict( dataset['new values'] )
 
#Multiple Linear regression, x's inputs may have the indexes being a 2D dataframe

 xs = dataset[['x1','x2','x3','x4','x5']]
 y = dataset['y']
 lm.fit( xs,y )
 result = pandas.DataFrame( {'Coefficients': lm.coef_, 'Intercept': lm.intercept_ , 'R2': lm.score(xs,y) } )

#Polynomial Regression of 3rd order

 import numpy as np
 from sklearn.metrics import r2_score
 
 x = dataset['x']
 y = dataset['y']
 f = np.polyfit( x , y , 3 )   #return an array with the 3 coefficients (highest first) and the intercept of the best fit
 y_predict = np.poly1d( f )    #return a numpy.poly1d equation 
 
 r2 = r2_score( y, y_predict(x) )
 result = pandas.DataFrame( {'Equation': y_predict , 'R2': r2} )
  
  
#Multiple Polynomial Regression of 2nd order 
#This one requires a bit more abstraction from linear algebra to understand
#Y = a + b1*x1 + b2*x2 + b3*x1x2 + b4*x1² + b5*x2² (for 2 variables -> 5 coefficients)
#First, we need to transform the features in polynomial equations before deploying the model

 xs = dataset[['x1','x2','x3','x4','x5']]
 
 from sklearn.linear_model import LinearRegression
 from sklearn.preprocessing import PolynomialFeatures
 pr = PolynomialFeatures(degree=2)
 xs_pr = pr.fit_transform( xs )
 
 #The features were transformed in new features with the order x, x2 and all combinations. Now we run a Linear Regression in this new dataframe
 lm = LinearRegression() 
 lm.fit( xs_pr , y )
 result = pandas.DataFrame( {'Coefficients': lm.coef_ , 'Intercept': lm.intercept_ , 'R2': lm.score(xs_pr,y) } )
 
#Pipeline to Multiple Polynomial Regression easier

 from sklearn.pipeline import Pipeline
 from sklearn.preprocessing import StandardScaler
 
 #We create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor.
 Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]
 pipe=Pipeline(Input)
 
 xs = dataset[['x1','x2','x3','x4','x5']]
 y = dataset['y']
 pipe.fit( xs , y )
 result = pandas.DataFrame( {'Coefficients': lm_coef_ , 'Intercept': lm_intercept_ , 'R2': lm_score(xs_pr,y) } )
 
#RidgeModel searchs to minimize the Least Squares, by damping the coefficients and avoid overfitting in the linear regression model

from sklearn.linear_model import Ridge
rr = Ridge(alpha = 0.1)
rr.fit(x_train , y_train)
return = rr.score(x_test, y_test)

#Non-linear regression (ex: logistic function)

def f(x, p1, p2):
 return 1 / (1+ np.exp(-p1*(x-p2))

from scipy.optimize import curve_fit
popt, pcov = curve_fit(f,x,y)
y_pred = f(x,*popt)

#CLASSIFICATION MODELS

#KNN - K Nearest neighbors

#Necessary to standardize (u=0, dp=1) because the model KNN is based in euclidean distance between features
from sklearn.preprocessing import StandardScaler
std = StandardScaler()
X = std.fit_transform(X)

#Other preprocessing method (scale down the number to 0=min 1=max, and do not change the numbers due to Standard Deviation)
from sklearn.preprocessing import MinMaxScaler
mm_scaler = MinMaxScaler()
X_minmax = mm_scaler.fit_transform(X)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = k)
knn.fit(X_train,y)
y_pred = knn.predict(X_test)

#Decision Tree - Classification model
from sklearn import tree
dtclf = tree.DecisionTreeClassifier()
dtclf.fit(X,y)
y_tree = dtclf.predict(X)

#Logistic Regression
from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(C=0.01, solver='liblinear')   #or leave empty for default settings
LR.fit(X_train,y_train)
y_logi = LR.predict(X_test)
y_logi_prob = LR.predict_proba(X_test)

#You may need to transform text classifications to numbers
from sklearn.preprocessing import LabelEconder()
le = LabelEnconder()
X = le.fit_transform(X)

#SVM Support Vector Machines
from sklearn import svm
svmclf = svm.SVC(kernel='rbf')
svmclf.fit(X_train, y_train) 
y_svm = svmclf.predict(X_test)

#CLUSTERING MODELS
#K-Means
from sklearn.cluster import KMeans 
k=3
k_means3 = KMeans(init = "k-means++", n_clusters = k, n_init = 12)
k_means3.fit(X)
fig = plt.figure(figsize=(6, 4))
ax = fig.add_subplot(1, 1, 1)
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
for k, col in zip(range(k), colors):
    my_members = (k_means3.labels_ == k)
    plt.scatter(X[my_members, 0], X[my_members, 1],  c=col, marker=u'o', alpha=0.5)
plt.show()

#Hierarchical
from sklearn.cluster import AgglomerativeClustering 
agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')
agglom.fit(df)
df['cluster'] = agglom.labels_
print(df.groupby('cluster').mean())

#DBSCAN - Density-Based Spacial Clustering of Applications with Noise
from sklearn.cluster import DBSCAN 
dbscan = DBSCAN(eps=0.3, min_samples=7).fit(X)   #Instance and fit in the same line
df['cluster'] = dbscan.labels_

####EVALUATE THE MODEL

#CLASSIFICATION MODELS

#Jaccard index (works for KNN, Decision Tree, Logistic Regression)
from sklearn.metrics import accuracy_score
clf_acc = accuracy_score(y,y_pred)
clf_acc_train = accuracy_score(y_train, y_pred)

#F1 Score
from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average='weighted') 

#Classification report
print('Classifier XYZ \n:', classification_report(y_test, y_pred)

#Confusion matrix
conda install -c districtdatalabs yellowbrick
from yellowbrick.classifier import confusion_matrix
from sklearn.metrics import classification_report

confusion_matrix(
    LogisticRegression(),                           #or any classifier model you would like
    X_train, y_train, X_test, y_test,
    classes=['categ1', 'categ2']
)
plt.tight_layout()
print (classification_report(y_test, y_pred))

#Print the Decision Tree
tree.plot_tree(dtclf).                                              #image
or
print(tree.export_text(dtclf, feature_names=X.columns.values)       #text
or
#!conda install -c conda-forge pydotplus -y
#conda install -c conda-forge python-graphviz -y                    #image export
from sklearn.externals.six import StringIO
import pydotplus
import matplotlib.image as mpimg
from sklearn import tree

dot_data = StringIO()
filename = "tree.png"
featureNames = X.columns
targetNames = df["Target"].unique().tolist()
out=tree.export_graphviz( clf , feature_names=featureNames, out_file=dot_data, class_names= np.unique(y_trainset), filled=True,  special_characters=True,rotate=False)  
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png(filename)
img = mpimg.imread(filename)
plt.figure(figsize=(100, 200))
plt.imshow(img,interpolation='nearest')


#REGRESSION METRICS
#Mean Absolute Error (MAE) 

MAE = np.mean( np.absolute(y - y_pred) )

#Mean Squared Error (MSE) 

 from sklearn.metrics import mean_squared_error
 MSE = mean_squared_error(dataset['y'], y_predict)

#R-squared (R²) 

 from sklearn.linear_model import LinearRegression
 LinearRegression.score(X,y)

#Mean Squared Error (MSE) 

 from sklearn.metrics import mean_squared_error
 MSE = mean_squared_error( dataset['y'], y_predict(x) )

#R-squared (R²) 

 from sklearn.metrics import r2_score
 r2 = r2_score( dataset['y'], y_predict(x) )
 
#Out-of-sample evaluations
#1 - Splitting the data in train and test samples

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(dataset['x'], dataset['y'], test_size=0.2, random_state=1)

#R2 for the test data

lr = LinearRegression()
lr.fit(x_train, y_train)
lr.score(x_test, y_test)

#Cross-validation samples (ex: cv=4)

from sklearn.model_selection import cross_val_score
from sklearn import metrics

scores = cross_val_score(lr, X, y, cv=4, scoring='f1' )   #OR any model you want instead of lr, scoring list: https://scikit-learn.org/stable/modules/model_evaluation.html
print('Score in cvs samples ', scores)
print('Score average ' scores.mean())
print('Score std deviation ' scores.std()} )
print('Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

#Using Grid Search to find the best hyperparameters like alpha for the Ridge 

from sklearn.model_selection import GridSearchCV
parameters = [{'alpha': [0.001, 0.1, 10, 100, 1000, 10000, 100000, 10000000]}]    #Add more if needed
rr=Ridge()
grid = GridSearchCV(rr, parameters, cv=4)
grid.fit(x_data, y_data)
rr = grid.best_estimator_

return = pandas.DataFrame( { 'Best model score': best_rr.score(x_test, y_test) } )


#GradientBoostingRegressor to optimize regressions
